{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 CUDA thread organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A grid is a three-dimensional array of blocks1, and each block is a three-dimensional array of threads**\n",
    "\n",
    "Configuration parameters within <<<```gridDim```, ```blockDim``` >>> are ```dim3``` type, \n",
    "which is a C ```struct``` with three unsigned integer fields: ```x, y, and z```\n",
    "\n",
    "\n",
    "**Exampe, n = 4000**\n",
    "```c\n",
    "dim3 dimGrid(ceil(n/256.0), 1, 1);\n",
    "dim3 dimBlock(256, 1, 1);\n",
    "vecAddKernel<<<dimGrid, dimBlock>>>;\n",
    "```\n",
    "\n",
    "In this example: \n",
    "* ```gridDim.x``` = 16,``` gridDim.y``` = 1, ```gridDim.z``` = 1\n",
    "* ```blockDim.x``` = 256, ```blockDim.y``` = 1, ```blockDim.z``` = 1\n",
    "\n",
    "\n",
    "**Allowable Ranges of gridDim and blockDim**\n",
    "\n",
    "* Allowed range of gridDim is from 1 to 65536\n",
    "* The total size of a block is limited to 1024 threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: multidimensional example of CUDA grid**\n",
    "\n",
    "```c\n",
    "dim3 dimGrid(2, 2, 1);\n",
    "dim3 dimBlock(4, 2, 2);\n",
    "vecAddKernel<<<dimGrid, dimBlock>>>;\n",
    "```\n",
    "The code above snippet generates the following\n",
    "\n",
    "<img src=\"resources/Fig3.1.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "## Order of index labels\n",
    "\n",
    "**a**. ```Grid``` consists of 4 blocks organized into a ```2 x 2``` array. Each ```block``` is labeled by ```(blockIdx.y, blockIdx, x)```. \n",
    "\n",
    "**Note:** the labels are ordered such the highest dimension comes first.\n",
    "* Note that this block labeling notation is the reversed ordering of that used in the C statements for setting configuration parameters where the lowest dimension comes first. \n",
    "* This reversed ordering for labeling blocks works more effectively when we illustrate the mapping of thread coordinates into data indexes in accessing multidimensional data.\n",
    "\n",
    "**b**.  Each ```Block``` consists of 4 x 2 x 2 arrays of threads. Similiar to ```block labeling```, threads can also be labeled by ```(threadIdx.z, threadIdx.y, threadIdx.x)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2. Mapping Threads to Multidimensional Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Problem: Using 2D grid consists of 2D blocks to process an 2D image\n",
    "\n",
    "Using 2D grid to process ```76 x 62``` picture ```P```.\n",
    "Assume blockDim = ```16 x 16 ```\n",
    "<img src=\"resources/Fig3.2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "* n_Block.x = gridDim.x = ceil(P.x / 16) = 5\n",
    "* n_Block.y = gridDim.y = ceil(P.y / 16) = 4\n",
    "\n",
    "Note there are 4 extra threads in x direction and 2 threads not used in y direction, ```if``` statements similiar to ```vecAdd``` helps test whether the thread are ```valid```.\n",
    "\n",
    "\n",
    "As a result: 20 blocks are needed. Pixel in ```thread(threadIdx.y, threadIdx.x)``` of ```block(blockId.y, blockId.x)``` could be derived using: \n",
    "* P(y, x) = P(```blockIdx.y * blockDim.y + threadIdx.y```, ```blockIdx.x * blockDim.x + threadIdx.x```)  \n",
    "\n",
    "For example: thread(0, 0) of block(1, 0) can be identified as:\n",
    "P(```1 * 16 + 0```, ```0 * 16 + 0```)\n",
    "\n",
    "**Kernel Function Declaration to process the picture**\n",
    "\n",
    "* Picture_input allocated on device can be accessed by pointer ```d_Pin``` \n",
    "* Picture_output allocated on device can be accessed by pointer ```d_Pout```\n",
    "```c\n",
    "dim3 dimGrid(ceil(m/16.0), ceil(n/16.0), 1);\n",
    "dim3 dimBlock(16, 16, 1);\n",
    "colorToGreyConversion<<<dimGrid, dimBlock>>>(d_Pin, d_Pout);\n",
    "```\n",
    "\n",
    "When processing a ```2000 x 1500``` picture: we get:\n",
    "\n",
    "* ```GridDim(125, 94, 1)```\n",
    "* ```BlockDim(16, 16, 1)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Access pixel ```d_Pin[j][i]```, i.e. Pixel at row ```j``` and col ```i```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:** CUDA requires explicitly flattening or lineraizing 2D array into equivalent 1D array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Row-major layout\n",
    "\n",
    "Placing all **elements of the same row** into **consecutive locations**. Next rows are placed one after another into the memory space\n",
    "\n",
    "<img src=\"resources/Fig3.3.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Lineraized ```M[j][i]``` could be accessed using ```M_flattened[j * width + i]```, i.e. by an index\n",
    "expression ```j*Width+i``` for an element that is in the ```jth``` row and ```ith``` column of an array of ```Width``` elements in each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Column-Major layout\n",
    "\n",
    "Placing all elements in the same column into consecutive locations. the columns are then placed one after another into the memory space. \n",
    "\n",
    "**The column-major layout of a 2D array is equivalent to the transposed row-major layout**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. ColorToGrey conversion code\n",
    "<img src=\"resources/Fig3.2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "<img src=\"resources/Fig3.4.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Pixel(```Row```, ```Col```)** location could be derived using:  \n",
    "$$ Col = threadIdx.x + blockIdx.x * blockDim.x   $$\n",
    "$$ Row = threadIdx.y + blockIdx.y * blockDim.y   $$\n",
    "\n",
    "\n",
    "**Flattened Pixel(```Row```, ```Col```)** could be derived using:\n",
    "$$ FlattendIdx = Row * Width + Col $$\n",
    "Where ```width``` is the **\"acutal\"** size of **x** direction\n",
    "\n",
    "For example: ```Thread(0, 0)```, ```Block(1, 0)```, ```blockDim```=16x16, ```GridDim```=5x4, ```ImgDim```=76x62 could be computed as:\n",
    "\n",
    "Where: ```threadIdx.x = threadIdx.y = 0```; ```BlockIdx.x = 0, BlockIdx.y = 1```\n",
    "\n",
    "$$ Row = 0 + 1 * 16 = 16; Col= 0 + 0 x 0 = 0 $$\n",
    "$$ Width = 76; FlattendIdx = 16 * 76 + 0 = 1216 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Extend to 3D arrays\n",
    "\n",
    "Given a ```m x n``` 2D array, adding an other dimension as **\"Plane\"**\n",
    "\n",
    "Now we have ```blockDim.z``` and ```gridDim.z```. Along with the corresponding ```threadIdx.z``` and ```BlockIdx.z```, then Pixel(Plane, Row, Col) could be defined as:\n",
    "$$Plane = blockIdx.z + blockDim.z + threadIdx.z$$\n",
    "$$ FlattenedIdx(3D) = Plane * Width * Height + Row * Width + Col $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Synchornization and transparent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.1 Scalability\n",
    "\n",
    "In CUDA, a ```barrier synchornization``` function ```__syncthreads()``` statement , if present, must be executed by all threads in a block. When a ```__syncthread()``` statement is placed in an ```if-statement```, either all or none of the threads in a block execute the path that includes the ```__syncthreads()```. \n",
    "\n",
    "For an ```if-then-else```: If each path has a ```__syncthreads()```, either all threads in a block execute ```then``` path or ```else``` path. Otherwise the program may wait for sync **forever**.\n",
    "\n",
    "As a result, all threads involved in the barrier synchronization have access to necessary resources must eventually arrive at the sync point. **CUDA satisfies this by assigning all threads in a block as a unit.**\n",
    "A block can begin excecution **only** when **all** resources are ready.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.2 Transparent Scalability in CUDA systems\n",
    "\n",
    "Above leads to the important design tradeoff of CUDA barrier sync: \n",
    "\n",
    "By **not** allowing threads in different blocks to perform ```barrier synchronization``` with each other, CUDA can execute blocks in **any** order relatuve to each other because none of them need to wait for each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/Fig3.11.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "                                  $$   Fig.3.11$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In higher-end systems, one can execute a large number of blocks simultaneously, as shown in RHS in Fig3.11. Lower-end system may execyte a smaller number at a time.\n",
    "\n",
    "This ability allows execute the same application with different number of hardware resources is referred to as  ***transparent scalability***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Resource Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in 3.4, threads are assigned to hardware execution resources on ```block-by-block``` basis.\n",
    "\n",
    "Currently, excecution resources are orgainized into **Streaming Multiprocessors (SMs)**. Each device sets a limit of blocks that can be assigned to each **SM**.\n",
    "\n",
    "For example, consider a CUDA device that allow 8 ```blocks```, the CUDA runtime automatically reduces the number of ```blocks``` assigned to each **SM** until their combined resource usage falls below the limit.\n",
    "\n",
    "The number of **SMs** and the number of ```blocks``` limits the number of actively excecuting ```blocks```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/Fig3.13.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, runtime system maintains a list of ```blocks``` that need to execute and assigns **new** ```blocks``` to **SMs** as **previously** assigned ```blocks``` complete execution. \n",
    "\n",
    "Fig above shows an example: Three ```blocks``` are assigned to each **SM**. \n",
    "* One of SM resource limitations is the number of threads that can be simutaneously tracked and scheduled. This limitation is determined by the hardware architecture: \n",
    " * For example: *Fermi* allows up to 8 ```blocks``` and 1536 ```threads``` for each **SM**.\n",
    " * If a device has 30 **SMs**, each SM follows *Fermi* constraints: Up to 46080 ```threads``` are allowed for simultaneously residing for excecution  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 Querying Device Properties: Several built-in mechanisms\n",
    "\n",
    "* ```cudaGetDeviceCount``` that returns the number of available CUDA devices in the system.\n",
    "```c\n",
    "int dev_count;\n",
    "cudaGetDeviceCount(&dev_count);\n",
    "\n",
    "```\n",
    "\n",
    "```cudaGetDeviceProperties``` returns the properties of the device whose number is given as an argument.\n",
    "```c\n",
    "cudaDeviceProp dev_prop;\n",
    "for(int i = 0; i < dev_count; i++) {\n",
    "    cudaGetDeviceProperties(&dev_prop, i);\n",
    "}\n",
    "```\n",
    "\n",
    "* built-in type ```cudaDeviceProp``` representing properties of a CUDA device. In the code above, ```dev_prop``` is the returned variable of ```cudaGetDeviceProperties(&dev_prop, i)```: several interesting fields are:\n",
    "* ```dev_prop.maxThreadsPerBlock```: the max number of threads allowed in a block\n",
    "* ```dev_prop.multiProcessorCount```: the number of SMs\n",
    "* ```dev_prop.maxThreadsDim[0], dev_prop.maxThreadsDim[1], dev_prop.maxThreadsDim[2]``` for ```x, y, z``` dimensions: the max number of threads allowed along each dimension of a block\n",
    "* ```dev_prop.maxGridSize[0], dev_prop.maxGridSize[1], dev_prop.maxGridSize[2]``` for ```x, y, z``` dimensions: the max number of blocks along each dimension of a grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.7. Thread Scheduling and Latency Tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays, a ```block``` assigned to an **SM** are is further divided into 32 threads units called **warps**. \n",
    "\n",
    "The size of warps is architecturally-specific and is a property of a CUDA device, which could be found in the ```dev_prop.warpSize``` field.\n",
    "\n",
    "The **warp** is the unit of thread scheduling in **SMs** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/Fig3.14.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. More of warps**\n",
    "\n",
    "\n",
    "As shown above, the division of blocks into warps. Each **warp** consists of 32 threads of consecutive ```threadIdx``` values: ```tidx0``` through ```tidx31``` form the 1st **warp**, ```tidx32``` through ```tidx63``` form the 2nd **warp**, and so on. \n",
    "\n",
    "In this examlpe, assume each block has 256 threads. Three blocks are assigned to am SM. Each of the three blocks is further divided into warps for scheduing purposes. Each ```block``` has 256/32=8 **warps**. In this SM, there are 3 x 8 = 24 **warps** in this SM. \n",
    "\n",
    "An SM is designed to executed all threads in a **warp** following the **SIMD** model - i.e., at any instant in time, one instruction is fetched and executed for all threads in the **warp**. Consequently, all threads in a **warp** will always have the same excecution timing.  \n",
    "\n",
    "**b. Warp scheduling**\n",
    "\n",
    "The figure also shows a number of hardware **Streaming Processors (SPs)** that **actually execute instructions**. Usually the number of **SPs** is much less than ```threads``` assigned to each **SM**; i.e., each SM only has enough HW to execute instructions from a small subset of all threads assigned to SM.\n",
    "\n",
    "Recent designs allow each SM execute instructions for a **small** number of warps at any point in time. The reason for a small number of processing units corresponding to large number of warps is the mechanism of **latency hiding**. **\"Bubbles\"** introduced by long-latency operations are hiden by switching to other **warps** that are ready for execution. \n",
    "\n",
    "### 3.7 Exercise \n",
    "Q: Assume a CUDA device allows 8 ```blocks``` and ```1024``` threads per **SM**. Additionally, it allows up to 512 threads in each block. For image blur, should we chose 8x8, 16x16 or 32x32 for ```blockDim```? *(Based on knowledge discussed in this chapter)*\n",
    "\n",
    "A: \n",
    "* ```8 x 8```: \n",
    " * each ```block``` gets 64 ```threads```. Then 1024 / 64 = 16 blocks are needed to fully occupy the SM. However, 16 > 8, each SM allows up to 8 blocks. 16 Blocks needs to be processed in 2 batches\n",
    " * Then each **SM** gets 64 x 8 = 512 threads. **SM** is not fully used (Only half of the processing ability is utilized). \n",
    " * Additionally, given a fixed **warp** size: this block gets 512/32 = 16 **warps**, a smaller number of warps makes it harder to hide long-latency operations\n",
    "\n",
    "\n",
    "* ```16 x 16```:\n",
    " * each ```block``` gets 256 ```threads```. Then 1024 / 256 = 4 blocks, 4 blocks is within the limitaion for SM simultaneous processing.\n",
    " * Each **SM** gets 256 x 4 = 1024 threads. 32 **Warps** is the maximal possible nubmer of warps, which can help hiding long latency ops.\n",
    "\n",
    "\n",
    "* ```32 x 32```:\n",
    "* Ez, 32 x 32 = 1024 >> 512, illegal to put 1024 threads in this device."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++11",
   "language": "C++11",
   "name": "xcpp11"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
